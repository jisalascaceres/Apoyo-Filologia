{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26642,"status":"ok","timestamp":1734294016162,"user":{"displayName":"Marcos Salas Pascual","userId":"01291233834475945794"},"user_tz":0},"id":"ZfwJo9Qhlkw1","outputId":"a0ca643f-3468-483e-856e-7c572a2411aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","tesseract-ocr is already the newest version (4.1.1-2.1build1).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libpoppler-cpp-dev is already the newest version (22.02.0-2ubuntu0.5).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n","Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n","Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n","Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.10.1)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","--2024-12-15 20:20:14--  https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa_old.traineddata\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9476925 (9.0M) [application/octet-stream]\n","Saving to: ‘spa_old.traineddata’\n","\n","spa_old.traineddata 100%[===================>]   9.04M  --.-KB/s    in 0.05s   \n","\n","2024-12-15 20:20:14 (181 MB/s) - ‘spa_old.traineddata’ saved [9476925/9476925]\n","\n","--2024-12-15 20:20:14--  https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa.traineddata\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13570187 (13M) [application/octet-stream]\n","Saving to: ‘spa.traineddata’\n","\n","spa.traineddata     100%[===================>]  12.94M  --.-KB/s    in 0.06s   \n","\n","2024-12-15 20:20:14 (204 MB/s) - ‘spa.traineddata’ saved [13570187/13570187]\n","\n"]}],"source":["# Instalamos Los paquetes necesarios.\n","!sudo apt install tesseract-ocr\n","!pip install pytesseract\n","!apt-get install poppler-utils\n","!pip install pdf2image\n","!apt-get install -y libpoppler-cpp-dev\n","!pip install --upgrade pdf2image\n","!pip install python-Levenshtein\n","!pip install pypdf\n","! wget https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa_old.traineddata #https://stackoverflow.com/questions/57968216/google-colab-how-do-i-install-traineddata-file-for-pytesseract\n","! sudo mv \"/content/spa_old.traineddata\" \"/usr/share/tesseract-ocr/4.00/tessdata\"\n","! wget https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa.traineddata #https://stackoverflow.com/questions/57968216/google-colab-how-do-i-install-traineddata-file-for-pytesseract\n","! sudo mv \"/content/spa.traineddata\" \"/usr/share/tesseract-ocr/4.00/tessdata\""]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1734294016162,"user":{"displayName":"Marcos Salas Pascual","userId":"01291233834475945794"},"user_tz":0},"id":"xqLyZXHmruBg","outputId":"5a6a5af7-7cfd-4527-b77d-68347e3d7fd9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}],"source":["import pandas as pd\n","import numpy as np\n","import sys\n","import random\n","import pypdf\n","from tqdm import tqdm\n","import os\n","import time\n","import Levenshtein\n","from nltk.tokenize import word_tokenize\n","import pytesseract\n","from pdf2image import convert_from_path\n","from pypdf import PdfReader\n","import nltk\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Kfwdpb55rww0","executionInfo":{"status":"ok","timestamp":1734294016481,"user_tz":0,"elapsed":327,"user":{"displayName":"Marcos Salas Pascual","userId":"01291233834475945794"}}},"outputs":[],"source":["def read_words(path):\n","    \"\"\"\n","    Esta funcion lee las palabras que se encuentran en la primera hoja de un excel y crea un dataframe con ellas y el count a cero.\n","    \"\"\"\n","    if path.endswith('.csv'):\n","        df = pd.read_csv(path,sep=',')\n","        words = df.values.tolist()\n","        words = [word[0] for word in words]\n","    elif path.endswith('.xlsx'):\n","        df = pd.read_excel(path, sheet_name = 0)\n","        words = df['Unnamed: 0'].tolist()\n","        #print(df)\n","    elif path.endswith('.txt'):\n","        with open(path, 'r') as file:\n","            words = file.readlines()\n","            # replace , and ; and . with \\n\n","            words = [word.replace(',','\\n').replace(';','\\n').replace('.','\\n') for word in words]\n","            # split by \\n\n","            words = [word.split('\\n') for word in words]\n","            # flatten the list\n","            words = [item for sublist in words for item in sublist]\n","            # remove empty strings\n","            words = [word for word in words if word != '']\n","\n","    else:\n","        print ('El archivo debe ser un csv, un excel o un txt.')\n","        return 1\n","        #raise ValueError(\"El archivo debe ser un csv, un excel o un txt.\")\n","\n","    for i in range(len(words)):\n","        words[i] = words[i].lower().replace(',','').replace('.','').replace(' ','')\n","\n","    count = np.zeros(len(words))\n","\n","    df_words = pd.DataFrame({'words':words, 'count':count})\n","    return df_words\n","\n","\n","def extract_images_from_pdf(pdf_path, start_page, end_page, poppler_path = None):\n","    \"\"\"\n","    Extract images from a PDF file.\n","\n","    Parameters:\n","    - pdf_path (str): Path to the PDF file.\n","    - start_page (int): Starting page number.\n","    - end_page (int): Ending page number.\n","\n","    Returns:\n","    - images (list): List of images.\n","    \"\"\"\n","    images = convert_from_path(pdf_path, dpi = 500, first_page = start_page, last_page = end_page,poppler_path = poppler_path) # poppler path is the path to the poppler bin folder\n","    return images\n","\n","\n","\n","def preprocess_images(list_of_images):\n","    \"\"\"\n","    Preprocess a list of images for OCR\n","\n","    Parameters:\n","    - list_of_images (list): List of images.\n","\n","    Returns:\n","    - images (list): List of images.\n","    \"\"\"\n","\n","    # convert images to grayscale\n","\n","    images = []\n","    for image in list_of_images:\n","        image = image.convert('L')\n","        images.append(image)\n","    return images\n","\n","\n","def find_substring_with_distance(input_string, target_word, max_distance):\n","    # This function finds the substring of input_string that has the minimum distance to target_word\n","    min_distance = float('inf')\n","    found_substring = None\n","\n","    for start in range(len(input_string)):\n","        for end in range(start + 1, len(input_string) + 1):\n","            substring = input_string[start:end]\n","            current_distance = levenshtein_distance(substring, target_word)\n","\n","            if current_distance < min_distance and current_distance <= max_distance:\n","                min_distance = current_distance\n","                found_substring = substring\n","\n","    return found_substring\n","\n","\n","def perform_ocr(image_path,language = 'spa_old'):\n","    text = pytesseract.image_to_string(image_path,lang = language)\n","    return text\n","\n","\n","def levenshtein_distance(word1, word2):\n","    return Levenshtein.distance(word1, word2)\n","\n","def count_words_with_levenshtein(words, target_word, max_distance=2,return_coincidences=False):\n","\n","    count = 0\n","    coincidences = []\n","    for word in words:\n","        word = find_substring_with_distance(word, target_word.lower(), max_distance)\n","        if word is not None:\n","            count += 1\n","            coincidences.append(word)\n","\n","    if return_coincidences:\n","        return count, coincidences\n","    else:\n","\n","        return count\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = text.replace(',',' ').replace('.',' ').replace('-\\n','').replace('\\n',' ')\n","    text = word_tokenize(text)\n","    return text\n","\n","\n","def preprocess_csv(path_csv):\n","    df = pd.read_csv(path_csv, index_col = 0)\n","    df = df.dropna()\n","    df['coincidences'] = df['coincidences'].str.split(',')\n","    df['Page'] = df['Page'].str.split(',')\n","    return df\n","\n","\n","from collections import Counter\n","\n","def combinar_listas(lista_palabras, lista_paginas):\n","    # Crear un diccionario para almacenar las páginas asociadas a cada palabra\n","    diccionario_paginas = {}\n","\n","    # Llenar el diccionario con las páginas asociadas a cada palabra\n","    for palabra, pagina in zip(lista_palabras, lista_paginas):\n","        if palabra in diccionario_paginas:\n","            diccionario_paginas[palabra]['paginas'].append(pagina)\n","            diccionario_paginas[palabra]['contador'] += 1\n","        else:\n","            diccionario_paginas[palabra] = {'paginas': [pagina], 'contador': 1}\n","\n","    # Crear la lista combinada\n","    lista_combinada = [f\"{palabra} ({', '.join(map(str, info['paginas']))}) [{info['contador']}]\" for palabra, info in diccionario_paginas.items()]\n","\n","    return lista_combinada"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"UoaIKqoBlmvL","executionInfo":{"status":"ok","timestamp":1734294016481,"user_tz":0,"elapsed":3,"user":{"displayName":"Marcos Salas Pascual","userId":"01291233834475945794"}}},"outputs":[],"source":["\n","\n","def Perform_word_count(path_pdf, path_csv, start_page = 1, end_page = -1, max_distance = 2, return_coincidences = True, path_output= None, formato = '.csv',\n","                       number_of_returns = 'Uno',folder = False,language = 'spa'):\n","    # Control de errores\n","\n","    # print ('---------------------------------------------------')\n","\n","    print ('Iniciando análisis de texto...')\n","    # print ('Parámetros:')\n","    print ('PDF: ', path_pdf)\n","    # print ('Archivo de entrada: ', path_csv)\n","    # print ('Página inicial: ', start_page)\n","    # print ('Página final: ', end_page)\n","    # print ('Distancia máxima: ', max_distance)\n","    # print ('Devolver coincidencias: ', return_coincidences)\n","    # print ('Archivo de salida: ', path_output)\n","    # print ('Formato de archivo de salida: ', formato)\n","    # print ('Número de retornos: ', number_of_returns)\n","    # print ('Modo folder?: ', folder)\n","\n","    # print ('---------------------------------------------------')\n","\n","\n","    if start_page < 1:\n","        print ('La página inicial debe ser mayor que 0, se establece el parámetro a 1.')\n","        start_page = 1\n","    if end_page <= start_page and end_page != -1:\n","        print ('La página final debe ser mayor que la inicial, puede insertar -1 para indicar que es la última página del documento.')\n","        return 1\n","        #raise ValueError(\"La página final debe ser mayor que la inicial, puede insertar -1 para indicar que es la última página del documento.\")\n","    if max_distance <= 0:\n","        print ('La distancia máxima debe ser mayor que 0.')\n","        return 1\n","        #raise ValueError(\"La distancia máxima debe ser mayor que 0.\")\n","    if path_output != None:\n","        if formato not in ['.csv','.xlsx','.txt']:\n","            print ('El formato de archivo de salida no es válido, se establece el formato a .csv')\n","            formato = '.csv'\n","\n","    # calculate the number of pages of the pdf\n","    with open(path_pdf, 'rb') as file:\n","        pdf_reader = PdfReader(file)\n","        num_pages = len(pdf_reader.pages)\n","\n","    if end_page > num_pages or end_page == -1:\n","        print ('La página final debe ser menor que el número de páginas del pdf, que es {}, se establece el parámetro a este último.'.format(num_pages))\n","        end_page = num_pages\n","\n","    print ('Número de páginas del PDF: ', num_pages)\n","    print ('Leyendo desde la página {} hasta la página {}'.format(start_page,end_page))\n","    #print ('Poppler path: ', poppler_path)\n","    # extract images from pdf\n","\n","    try:\n","        images = convert_from_path(path_pdf, size = 800, first_page = start_page, last_page=end_page,poppler_path='/usr/bin')\n","    except Exception as e:\n","        #print the error\n","        print (e)\n","        print ('Error al extraer las imágenes del PDF.')\n","        return 1\n","        #raise ValueError(\"Error al extraer las imágenes del PDF.\")\n","    images = preprocess_images(images)\n","    text = ''\n","\n","    print ('Digitalizando el texto...')\n","    print ('Esto puede tardar unos minutos, por favor, espere.')\n","    page_counter = start_page\n","    book = []\n","    #print ('tesseract path: ', tesseract_path)\n","    for image in tqdm(images):\n","        try:\n","            text = perform_ocr(image,language=language)\n","        except Exception as e:\n","            #print the error\n","            print (e)\n","            print ('Error al realizar OCR.')\n","            return 1\n","            #raise ValueError(\"Error al realizar OCR.\")\n","        book.append([page_counter,text])\n","        page_counter += 1\n","\n","    print('Buscando palabras...')\n","\n","    try:\n","        df_words = read_words(path_csv)\n","    except:\n","        print ('Error al leer el archivo de entrada de palabras.')\n","        return 1\n","        #raise ValueError(\"Error al leer el archivo de entrada.\")\n","\n","    for i in range(len(book)):\n","        book[i][1] = preprocess_text(book[i][1])\n","\n","\n","    if return_coincidences:\n","        df_words['coincidences'] = np.nan\n","        df_words['Page'] = np.nan\n","\n","    for index, row in df_words.iterrows():\n","        if return_coincidences:\n","            counter = 0\n","            coincidences = []\n","            page_numbers = []\n","            for page in book:\n","                count,coincidence = count_words_with_levenshtein(page[1], row['words'], max_distance,return_coincidences)\n","                counter += count\n","                coincidences.extend(coincidence)\n","                page_numbers.extend([page[0]]*len(coincidence))\n","\n","\n","            coincidences = np.array(coincidences)\n","                # Convertir el array de coincidencias en un string\n","            coincidences = ','.join(coincidences)\n","                # return the page where the word was found\n","            page_numbers = np.array(page_numbers)\n","            page_numbers = page_numbers.astype(str)\n","            page_numbers = ','.join(page_numbers)\n","\n","            df_words.loc[index, 'Page'] = page_numbers\n","            df_words.loc[index, 'count'] = counter\n","            df_words.loc[index, 'coincidences'] = coincidences\n","\n","        else:\n","\n","            for page in book:\n","                count = 0\n","                count = count_words_with_levenshtein(page[1], row['words'], max_distance,return_coincidences)\n","                count += count\n","            df_words.loc[index, 'count'] = count\n","\n","\n","    print ('Guardando resultados...')\n","    df_words = df_words.dropna()\n","    df_new = pd.DataFrame(columns = ['Palabra','Coincidencias exactas','Coincidencias no exactas'])\n","    for index, row in df_words.iterrows():\n","        non_exact = []\n","        coincidentes = row['coincidences']\n","        coincidentes = coincidentes.split(',')\n","        pages = row['Page']\n","        pages = pages.split(',')\n","\n","        coincidencias = combinar_listas(coincidentes,pages)\n","\n","        df_new.loc[index,'Palabra'] = row['words']\n","\n","        for coincidencia in coincidencias:\n","            objetivo = df_words.loc[index, 'words']\n","            #print ('Objetivo:',objetivo)\n","            #print (coincidencias)\n","\n","            palabra = coincidencia.split(' ')[0]\n","            if levenshtein_distance(palabra, objetivo) == 0:\n","                df_new.loc[index,'Coincidencias exactas'] = coincidencia\n","\n","            else:\n","                non_exact.append(coincidencia)\n","\n","        if number_of_returns == 'Todas':\n","            df_new.loc[index,'Coincidencias no exactas'] = non_exact\n","        elif number_of_returns == 'Uno':\n","            if len(non_exact) > 0:\n","                df_new.loc[index,'Coincidencias no exactas'] = random.choice(non_exact)\n","            else:\n","                df_new.loc[index,'Coincidencias no exactas'] = ''\n","        elif number_of_returns == 'Dos':\n","            if len(non_exact) > 0:\n","                if len(non_exact) < 2:\n","                    df_new.loc[index,'Coincidencias no exactas'] = non_exact\n","                else:\n","                    aux = []\n","                    aux.append(random.choice(non_exact))\n","                    aux.append(random.choice(non_exact))\n","                    df_new.loc[index,'Coincidencias no exactas'] = aux\n","\n","\n","            else:\n","                df_new.loc[index,'Coincidencias no exactas'] = ''\n","\n","        else:\n","            # Else es Ninguno\n","            df_new.loc[index,'Coincidencias no exactas'] = ''\n","\n","\n","\n","\n","    # if you put the format also in the name of the file, delete it.add()\n","\n","    if path_output != None:\n","        if '.' in path_output:\n","            path_output = path_output.split('.')[:-1]\n","            path_output = '.'.join(path_output)\n","            path_output = path_output\n","    else:\n","        path_output = 'output'\n","\n","    # if we are perfoming the analysis for a folder, we add the name of the pdf to the output file, to avoid overwriting\n","\n","    if folder:\n","        path_output = path_output+'_'+ path_pdf.split('\\\\')[-1].split('.')[0]\n","\n","    # save the file in the desired format\n","\n","    if formato == '.csv':\n","        df_new.to_csv(path_output + '.csv')\n","\n","    if formato == '.xlsx':\n","        df_new.to_excel(path_output+'.xlsx')\n","\n","    if formato == '.txt':\n","        df_new.to_csv(path_output+ '.txt',sep='\\t')\n","\n","    print ('Archivo guardado en: ', path_output + formato)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWv2OFuPmD7c","outputId":"824b2581-fad4-46fb-c7f4-82c893b17833","executionInfo":{"status":"ok","timestamp":1734294943887,"user_tz":0,"elapsed":927408,"user":{"displayName":"Marcos Salas Pascual","userId":"01291233834475945794"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando análisis de texto...\n","PDF:  /content/1637-Moreno.pdf\n","La página final debe ser menor que el número de páginas del pdf, que es 73, se establece el parámetro a este último.\n","Número de páginas del PDF:  73\n","Leyendo desde la página 1 hasta la página 73\n","Digitalizando el texto...\n","Esto puede tardar unos minutos, por favor, espere.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 73/73 [14:01<00:00, 11.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Buscando palabras...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-6c2c44de1284>:118: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '3,3,3,3,4,4,4,6,7,7,7,7,7,7,7,7,7,8,8,8,8,8,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,18,18,18,18,18,19,19,19,19,19,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,23,23,23,23,24,24,24,24,25,25,25,25,25,26,26,26,26,26,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,35,35,35,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,36,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,43,43,43,43,43,43,44,44,44,44,44,44,44,44,44,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,46,46,46,46,46,46,46,46,46,46,46,46,46,46,47,47,47,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,50,50,50,50,51,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,52,52,53,53,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,58,59,59,59,59,59,60,60,60,60,60,60,61,61,61,61,62,62,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,68,69,69,69,69,69,69,69,69,69,69,69,70,70,70,70,70,70,70,70,70,70,70,71,71,71,71,71,71,71,72,72' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n","  df_words.loc[index, 'Page'] = page_numbers\n","<ipython-input-14-6c2c44de1284>:120: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'alama,alama,achma,alama,alama,alama,alama,spana,axaca,ataca,axaca,ican,nfana,acon,icara,acon,arcina,axaca,amina,icada,ican,scan,axaca,axaca,sana,adan,axaca,aca,alcan,umana,icada,acada,alcan,axaca,ican,aca,acaba,alaba,icada,adan,acon,can,agan,cada,acta,ayan,scan,agan,gana,gana,scan,cada,acha,atan,acha,ayan,cada,acha,agan,amena,aman,acra,acin,aca,mana,casa,casa,atina,acra,añan,aca,acusa,acusa,acin,ncapa,acon,añan,adan,octna,can,can,acon,ecan,acaba,acon,acon,ana,alara,ecan,ayan,can,aran,agan,ajan,cada,ayan,cada,casa,casa,cada,acon,alaba,casa,alaba,acon,aran,casa,caca,agan,aca,cata,agan,casa,casa,casa,casa,ajena,gana,gana,casa,_casa,ocan,can,aran,aciona,gana,lcan,acen,scata,scata,auan,casa,aca,vana,anina,acon,scan,grana,grana,trana,allana,grana,grana,agara,rana,acta,grana,grana,grana,grana,gana,rcan,casa,scan,cada,can,aca,agan,agan,agan,caua,agan,casa,acia,cona,acan,ayan,gana,can,llana,asan,cona,acia,rcada,can,ican,agan,agan,acha,can,grana,grana,enana,cada,can,grana,grana,acin,grana,gana,aca,agaa,ataua,aca,agan,aca,can,aran,casa,aran,agan,caua,aran,azan,atan,cara,arata,scan,aca,alan,rcan,lcada,icada,can,aca,aman,abona,anana,aca,casa,casa,casa,acan,acin,acin,amana,cona,caua,agan,ecan,agan,amara,agan,asan,cada,gana,aca,aran,agan,ncapa,ecan,acin,aaba,enana,amena,casa,acin,itana,casa,gana,acena,aman,aman,ocan,auan,can,can,umana,erana,umana,atan,amna,umana,umana,stana,can,atan,atan,tana,can,icada,casa,atan,agina,cava,casa,cana,can,agan,pana,ecaa,acusa,aan,acon,casa,gana,ana,can,gana,capa,ecan,grana,caca,agina,gana,can,casa,caca,caca,cada,añara,caca,caca,casa,caca,caca,caca,caca,caca,caca,lana,aran,cada,casa,can,caga,can,ayan,can,capa,gana,can,can,acuda,casa,nana,caba,pana,vana,casa,cada,cada,casa,cada,gana,casa,cada,aca,aca,cada,acusa,acusa,acusa,icada,cada,ecan,acota,_cada,amena,amena,cada,casa,cada,cada,aca,acan,casa,gana,casa,aran,aciona,caa,tana,aciona,can,casa,scan,alan,cada,cada,emana,cava,gana,aca,gana,can,casa,aca,cava,asan,cada,aca,caua,aca,aca,caua,caba,can,agan,cada,aca,can,caua,aca,aca,cava,caua,aca,aca,_caua,aca,caua,aca,caua,aca,caua,arada,can,aca,aca,aca,aca,aca,emana,emana,gana,gana,aca,ayan,casa,ayan,abaja,casa,ecan,amina,ayan,can,agan,cada,ataja,icna,acen,acata,aran,acia,icina,acen,lana,aman,can,cona,diana,cama,can,acta,can,actua,aran,can,asan,casa,acra,can,actua,acha,alaa,alaa,ayan,scan,agena,acia,scan,actua,rcan,rcan,rcan,casa,axaca,auan,aca,cada,actua,rcan,caua,cada,scan,rcan,cada,acia,grana,caca,grana,alaja,caca,agaua,can,caca,caca,axaca,aman,can,caia,grana,agan,auan,grana,aca,casa,grana,grana,grana,acaba,ntana,cama,aran,cona,can,rcan,can,atan,icna,sana,aca,actua,casa,sana,can,cada,can,aca,cana,avena,alan,cena,cada,acia,aman,auan,alcan,aca,ocan,ecena,mana,acin,araa,acon,ecan,ecan,añan,acon,acen,acon,aran,cada,cada,aca,actua,alcan,añan,cona,amena,apan,dana,scan,capa,ccan,acta,apan,ecan,acita,ayan,cada,ican,atana,can,capa,atana,agara,acita,añada,acepa,acon,ncada,alcaba,agan,acia,amena,agena,amena,acha,acha,acaba,acha,acia,acra,acra,scan,can,can,ncapa,cada,acra,azona,acra,ccata,acra,can,acra,casa,ican,icina,ecara,acota,acusa,ican,acon,acon,acon,can,acon,acon,acea,acen,acea,acon,can,acon,ncan,agan,alcan,ayan,aca,can,capa,casa,aca,ican,acia,acia,cada,acia,acia,ican,acia,acia,acia,acia,can,can,atada,aran,aca,gana,atan,scan,acin,agan,aca,cada,atan,acra,aman,acia,alcaua,atan,atan,acaba,acia,grana,umana,agan,agan,ofana,amena,can,atada,scan,scan,scan,can,scan,scan,añan,scan,scan,can,scan,auan,scan,scan,scan,scan,ecan,scan,ecan,scan,umana,acia,agan,scan,cada,stana,agan,ajan,acan,añan,ecan,acn,isana,aman,scan,scan,ofana,acra,agena,atan,acon,acaba,acaba,abona,araña,cada,sana,ncan,ayan,scan,aran,sana,atara,acia,aan,ican,ofana,ccata' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n","  df_words.loc[index, 'coincidences'] = coincidences\n"]},{"output_type":"stream","name":"stdout","text":["Guardando resultados...\n","Archivo guardado en:  Output.xlsx\n"]}],"source":["name_excel = 'antillanismos.xlsx'\n","name_pdf = '1637-Moreno.pdf'\n","path_output = 'Output.xlsx'\n","\n","path_csv = '/content/' + name_excel\n","path_pdf = '/content/' + name_pdf\n","\n","start_page = 1\n","end_page = -1\n","max_distance = 2\n","return_coincidences = True\n","formato = '.xlsx'\n","number_of_returns = 2\n","folder = False\n","\n","\n","\n","Perform_word_count(path_pdf, path_csv, start_page, end_page, max_distance, return_coincidences, path_output, formato,\n","                       number_of_returns, folder,'spa')\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}