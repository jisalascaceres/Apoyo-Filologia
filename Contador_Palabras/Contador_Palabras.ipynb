{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNFPRWHbBsBn"
      },
      "source": [
        "### Contador de palabras.\n",
        "\n",
        "Este código esta diseñado para realizar el conteo de determinadas palabras en un PDF aplicando un OCR, en concreto tesseract.\n",
        "\n",
        "El listado de palabras se inserta con un archivo csv, txt o excel. Más abajo hay más información al respecto.\n",
        "\n",
        "Para un uso habitual del código, hay que subir ambos archivos, PDF y archivo de palabras, a la carpeta de Colab, situada a la izquierda. Posteriormente se cambian los parámetros pertinentes en la última celda y en la pestaña de \"entorno de ejecución\" se da a \"ejecutar todas\".\n",
        "\n",
        "El excel resultante tendrá tres columnas.\n",
        "- Palabra: Donde estará la palabra que se estaba buscando.\n",
        "- Coincidencia exacta: Donde estarán las coincidencias exactas y dos números. Las páginas donde se encontraron las coincidencias, entre parentesis y la cantidad de veces que aparece en total, entre brackets. Ej. estudiante(1,2) [3]. Esto significa que se encontro la palabra estudiante en las páginas 1 y 2, y en total se encontró 3 veces.\n",
        "- Coincidencias no exactas: Palabra parecia a la de objetivo encontrada junto con el número de la página en la que se encontró\n",
        "\n",
        "El código ha sido creado por José Ignacio Salas Cáceres. Correo de contacto: jose.salas@ulpgc.es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfwJo9Qhlkw1",
        "outputId": "b13af659-7bf4-479d-a92d-cd7fb8d85ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,587 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (439 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123680 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpoppler-cpp0v5\n",
            "The following NEW packages will be installed:\n",
            "  libpoppler-cpp-dev libpoppler-cpp0v5\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 50.4 kB of archives.\n",
            "After this operation, 231 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-cpp0v5 amd64 22.02.0-2ubuntu0.5 [38.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-cpp-dev amd64 22.02.0-2ubuntu0.5 [11.7 kB]\n",
            "Fetched 50.4 kB in 0s (156 kB/s)\n",
            "Selecting previously unselected package libpoppler-cpp0v5:amd64.\n",
            "(Reading database ... 123710 files and directories currently installed.)\n",
            "Preparing to unpack .../libpoppler-cpp0v5_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking libpoppler-cpp0v5:amd64 (22.02.0-2ubuntu0.5) ...\n",
            "Selecting previously unselected package libpoppler-cpp-dev:amd64.\n",
            "Preparing to unpack .../libpoppler-cpp-dev_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking libpoppler-cpp-dev:amd64 (22.02.0-2ubuntu0.5) ...\n",
            "Setting up libpoppler-cpp0v5:amd64 (22.02.0-2ubuntu0.5) ...\n",
            "Setting up libpoppler-cpp-dev:amd64 (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.26.1 python-Levenshtein-0.26.1 rapidfuzz-3.10.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=7a017c257c7b061b36dee4e2e577ce71aba040d70d23c02e22a536afdfacf5eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n",
            "--2024-12-17 14:07:53--  https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa_old.traineddata\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9476925 (9.0M) [application/octet-stream]\n",
            "Saving to: ‘spa_old.traineddata’\n",
            "\n",
            "spa_old.traineddata 100%[===================>]   9.04M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-12-17 14:07:53 (111 MB/s) - ‘spa_old.traineddata’ saved [9476925/9476925]\n",
            "\n",
            "--2024-12-17 14:07:54--  https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa.traineddata\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13570187 (13M) [application/octet-stream]\n",
            "Saving to: ‘spa.traineddata’\n",
            "\n",
            "spa.traineddata     100%[===================>]  12.94M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-12-17 14:07:54 (137 MB/s) - ‘spa.traineddata’ saved [13570187/13570187]\n",
            "\n",
            "--2024-12-17 14:07:54--  https://raw.githubusercontent.com/jisalascaceres/Apoyo-Filologia/main/Contador_Palabras/Word_count.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11083 (11K) [text/plain]\n",
            "Saving to: ‘Word_count.py’\n",
            "\n",
            "Word_count.py       100%[===================>]  10.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-17 14:07:55 (50.9 MB/s) - ‘Word_count.py’ saved [11083/11083]\n",
            "\n",
            "--2024-12-17 14:07:55--  https://raw.githubusercontent.com/jisalascaceres/Apoyo-Filologia/main/Contador_Palabras/Utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6522 (6.4K) [text/plain]\n",
            "Saving to: ‘Utils.py’\n",
            "\n",
            "Utils.py            100%[===================>]   6.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-17 14:07:55 (43.0 MB/s) - ‘Utils.py’ saved [6522/6522]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalamos Los paquetes necesarios.\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!apt-get install -y libpoppler-cpp-dev\n",
        "!pip install --upgrade pdf2image\n",
        "!pip install python-Levenshtein\n",
        "!pip install pypdf\n",
        "!pip install fpdf\n",
        "! wget https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa_old.traineddata #https://stackoverflow.com/questions/57968216/google-colab-how-do-i-install-traineddata-file-for-pytesseract\n",
        "! sudo mv \"/content/spa_old.traineddata\" \"/usr/share/tesseract-ocr/4.00/tessdata\"\n",
        "! wget https://raw.githubusercontent.com/tesseract-ocr/tessdata_best/master/spa.traineddata #https://stackoverflow.com/questions/57968216/google-colab-how-do-i-install-traineddata-file-for-pytesseract\n",
        "! sudo mv \"/content/spa.traineddata\" \"/usr/share/tesseract-ocr/4.00/tessdata\"\n",
        "\n",
        "# Descargamos los scripts con las funciones necesarias de github https://github.com/jisalascaceres/Apoyo-Filologia\n",
        "! wget https://raw.githubusercontent.com/jisalascaceres/Apoyo-Filologia/main/Contador_Palabras/Word_count.py\n",
        "! wget https://raw.githubusercontent.com/jisalascaceres/Apoyo-Filologia/main/Contador_Palabras/Utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqLyZXHmruBg",
        "outputId": "d4c1ab05-4794-4f97-d11a-eaff38244e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import pypdf\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import Levenshtein\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from pypdf import PdfReader\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from fpdf import FPDF\n",
        "\n",
        "from Utils import *\n",
        "from Word_count import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emy5LwFy5VW7"
      },
      "source": [
        "### Parametros:\n",
        "- **name_excel**: Nombre del archivo de entrada con las palabras a buscar. Puede ser .csv, .txt o excel. Más abajo hay más información sobre el formato.\n",
        "- **name_pdf**: Nombre del archivo pdf del que se va a realizar el conteo de palabras.\n",
        "- **Página inicial**: Página del PDF desde la que se quiere empezar a contar.\n",
        "- **Página final**: Página del PDF en la que se quiere terminar de contar. Si se pone -1, automáticamente coge la última página del PDF.\n",
        "- **Distancia**: Distancia máxima entre dos palabras para guardar coincidencias no exactas. La distancia se calcula como el número de ediciones que hay que realizar a la palabra para que se convierta en otra. Para más información, busca \"distancia Levenshtein\".\n",
        "- **Nombre archivo resultante**: Nombre del archivo csv que se creará con los resultados.\n",
        "- **Formato**: Formato del archivo resultante. (.xlsx, .csv o .txt)\n",
        "- **Aplicar_OCR** = Si hace falta o no aplicar OCR al PDF. Si el archivo esta digitalizado ya, no hará falta hacerlo.\n",
        "- **Guardar_Text** = En caso de aplicar OCR, si quieres guardar el texto digitalizado por el OCR en otro PDF. Este nuevo PDF respetará la página en la que se ha encuentra cada palabra, pero no el formato exacto. Será texto plano.\n",
        "- **Buscar_substrings** = Si quieres buscar la palabra dentro de otras palabras detectadas. Esto aporta robustez a no detectar un espacio. O puede detectar palabras compuestas.\n",
        "- **Aplicar_Conteo** = Si se quiere contar las palabras o solo realizar el OCR.\n",
        "\n",
        "\n",
        "Formato de archivo entrada:\n",
        " - En el caso de un .csv, el archivo debe tener una sola columna con palabras a buscar separadas por espacios.\n",
        " - En el caso de un excel (.xlsx), el archivo debe tener una sola columna con las palabras en la primera página.\n",
        " - En el caso de un .txt, el archivo debe contener todas las palabras separadas por comas, puntos o la letra enter (retorno de carro)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWv2OFuPmD7c",
        "outputId": "8a93ded4-bdef-4da9-fa9f-c0babc6843ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando análisis de texto...\n",
            "PDF:  /content/hola.pdf\n",
            "Archivo de entrada:  /content/Hispanismos.xlsx\n",
            "La página final debe ser menor que el número de páginas del pdf, que es 2, se establece el parámetro a este último.\n",
            "Número de páginas del PDF:  2\n",
            "Leyendo desde la página 1 hasta la página 2\n",
            "Convirtiendo el PDF a Imágenes para aplicar el OCR.\n",
            "Digitalizando el texto...\n",
            "Esto puede tardar unos minutos, por favor, espere.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:24<00:00, 12.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardando el texto digitalizado...\n",
            "PDF saved as: output.pdf\n",
            "Buscando palabras...\n",
            "Iterando por la lista de palabras. Buscandolas en el texto. Esto puede tardar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:00, 464.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardando resultados...\n",
            "Archivo guardado en:  Output.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "name_excel = 'Hispanismos.xlsx'\n",
        "name_pdf = 'hola.pdf'\n",
        "nombre_archivo_resultante = 'Output.xlsx'\n",
        "\n",
        "path_csv = '/content/' + name_excel\n",
        "path_pdf = '/content/' + name_pdf\n",
        "\n",
        "Pagina_inicial = 1\n",
        "Pagina_final = -1\n",
        "Distancia = 2\n",
        "\n",
        "Aplicar_OCR = True\n",
        "Guardar_Texto = True\n",
        "Buscar_substrings = False\n",
        "Aplicar_Conteo = True\n",
        "formato = '.xlsx'\n",
        "\n",
        "\n",
        "\n",
        "Perform_word_count(path_pdf, path_csv, Pagina_inicial, Pagina_final, Distancia, nombre_archivo_resultante, formato\n",
        "                   ,'spa',Aplicar_OCR=Aplicar_OCR,save_text=Guardar_Texto,Aplicar_Conteo = Aplicar_Conteo,Buscar_substrings=Buscar_substrings)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}